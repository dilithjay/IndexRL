{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = list(\"()+-*/=\") + [\"sq\", \"sqrt\"] + [f\"c{c}\" for c in range(13)]\n",
    "\n",
    "def state_to_exp(state):\n",
    "    return list(map(lambda x: action_list[x], state))\n",
    "\n",
    "def exp_to_state(exp):\n",
    "    return list(map(lambda x: action_list.index(x), exp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUCs to seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_path = \"cache/cache-cloud/seen.pkl\"\n",
    "seen_set = set()\n",
    "if os.path.exists(seen_path):\n",
    "    with open(seen_path, \"rb\") as fp:\n",
    "        seen_set = pickle.load(seen_path)\n",
    "\n",
    "auc_paths = [\"logs/logs-cloud/aucs.txt\", \"logs/logs-cloud-1/aucs-1.txt\", \"logs/logs-cloud-2/aucs.txt\"]\n",
    "\n",
    "tot_count = 0\n",
    "for path in auc_paths:\n",
    "    with open(path, \"r\") as fp:\n",
    "        lines = fp.read().split(\"\\n\")[:-1]\n",
    "    tot_count += len(lines)\n",
    "    for line in lines:\n",
    "        exp = line.split(\" \", 2)[-1]\n",
    "        seen_set.add(exp)\n",
    "\n",
    "print(len(seen_set), tot_count)\n",
    "with open(seen_path, \"wb\") as fp:\n",
    "    pickle.dump(seen_set, fp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUCs to Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/dilith/Projects/IndexRL/logs/logs-cloud/aucs.txt\", \"r\") as fp:\n",
    "    lines = fp.read().split(\"\\n\")[:-1]\n",
    "\n",
    "data_buffer = []\n",
    "\n",
    "for line in lines:\n",
    "    elems = line.split()\n",
    "    expression = eval(\"\".join(elems[2:]))\n",
    "    data_buffer.append((exp_to_state(expression), float(elems[1])))\n",
    "\n",
    "with open(f\"/home/dilith/Projects/IndexRL/cache/cache-cloud/data_buffer_0.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(data_buffer, fp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Cache Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_reward import get_training_based_reward\n",
    "\n",
    "\n",
    "cache_dir = \"cache/cache-cloud-rew\"\n",
    "image_dir = \"/home/dilith/Projects/SerpSeg/data/subscenes/\"\n",
    "mask_dir = \"/home/dilith/Projects/SerpSeg/data/masks/\"\n",
    "cache_paths = glob(os.path.join(cache_dir, \"data_buffer*.pkl\"))\n",
    "\n",
    "seen = set()\n",
    "new_cache = []\n",
    "for path in cache_paths:\n",
    "    with open(path, \"rb\") as fp:\n",
    "        cache = pickle.load(fp)\n",
    "    for state, _ in tqdm(cache):\n",
    "        exp = state_to_exp(state)\n",
    "        if len(exp) < 3:\n",
    "            continue\n",
    "        reward = get_training_based_reward(exp, image_dir, mask_dir)\n",
    "        if str(exp) in seen:\n",
    "            continue\n",
    "        new_cache.append((state, reward))\n",
    "        seen.add(str(exp))\n",
    "\n",
    "with open(os.path.join(cache_dir, \"data_buffer_0.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(new_cache, fp)\n",
    "\n",
    "with open(os.path.join(cache_dir, \"seen.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(seen, fp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache to AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = \"cache/cache-cloud-rew/data_buffer_00.pkl\"\n",
    "auc_path = \"logs/logs-cloud-rew/aucs.txt\"\n",
    "\n",
    "with open(cache_path, 'rb') as fp:\n",
    "    cache = pickle.load(fp)\n",
    "    \n",
    "with open(auc_path, 'w') as fp:\n",
    "    for state, reward in cache:\n",
    "        fp.write(f\"{reward} {state_to_exp(state)}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache exp to state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 63329.08it/s]\n"
     ]
    }
   ],
   "source": [
    "cache_path = \"cache/cache-cloud-rew/data_buffer_0.pkl\"\n",
    "new_cache_path = \"cache/cache-cloud-rew/data_buffer_00.pkl\"\n",
    "\n",
    "with open(cache_path, 'rb') as fp:\n",
    "    cache = pickle.load(fp)\n",
    "\n",
    "new_cache = []\n",
    "for exp, reward in tqdm(cache):\n",
    "    new_cache.append((exp_to_state(exp), reward))\n",
    "\n",
    "with open(new_cache_path, 'wb') as fp:\n",
    "    pickle.dump(new_cache, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
